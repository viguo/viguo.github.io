[TOC]()

# 前提条件和准备工作

在开始机器学习速成课程之前，请先阅读下面的前提条件和准备工作部分，以确保您已做好完成所有单元所需的准备工作。

## 前提条件
机器学习速成课程并不会假定或要求您预先掌握机器学习方面的任何知识。但是，为了能够理解课程中介绍的概念并完成练习，您最好满足以下前提条件：

 - **掌握入门级代数知识**， 您应该了解变量和系数、线性方程式、函数图和直方图（熟悉对数和导数等更高级的数学概念会有帮助，但不是必需条件）。
 - **熟练掌握编程基础知识**，并且具有一些使用 Python 进行编码的经验。 机器学习速成课程中的编程练习是通过 TensorFlow 并使用 Python 进行编码的。您无需拥有任何 TensorFlow 经验，但应该能够熟练阅读和编写包含基础编程结构（例如，函数定义/调用、列表和字典、循环和条件表达式）的 Python 代码。

## 准备工作
### Pandas 使用入门
机器学习速成课程中的编程练习使用 Pandas 库来操控数据集。如果您不熟悉 Pandas，建议您先学习Pandas 简介教程，该教程介绍了练习中使用的主要 Pandas 功能。
#### 低阶 TensorFlow 基础知识
机器学习速成课程中的编程练习使用 TensorFlow 的高阶 tf.estimator API 来配置模型。如果您有兴趣从头开始构建 TensorFlow 模型，请学习以下教程：

 - TensorFlow Hello World：在低阶 TensorFlow 中编码的“Hello World”。
 - TensorFlow 编程概念：演示了 TensorFlow 应用中的基本组件：张量、指令、图和会话。
 - 创建和操控张量：张量快速入门 - TensorFlow 编程中的核心概念。此外，还回顾了线性代数中的矩阵加法和乘法概念。
#### 主要概念和工具
机器学习速成课程中介绍并应用了以下概念和工具。有关详情，请参阅链接的资源。
##### 数学
 - **代数**
   - 变量、系数和函数
   - 线性方程式，例如 $y = b + w_1x_1 + w_2x_2$
   - 对数和对数方程式，例如 $y = ln(1+ e^z)$
   -  S 型函数

 - **线性代数**
    - 张量和张量等级
    - 矩阵乘法
 - **三角学**
   - Tanh（作为激活函数进行讲解，无需提前掌握相关知识）
 - **统计信息**
   - 均值、中间值、离群值和标准偏差
   - 能够读懂直方图 
 - **微积分（可选，适合高级主题）**
   - 概念（您不必真正计算导数）
   - 梯度或斜率
   - 偏导数（与梯度紧密相关）
   - 链式法则（带您全面了解用于训练神经网络的反向传播算法）
#### Python 编程
- **基础 Python**
  - Python 教程中介绍了以下 Python 基础知识：
  - 定义和调用函数：使用位置和关键字参数
  - 字典、列表、集合（创建、访问和迭代）
  - for 循环：包含多个迭代器变量的 for 循环（例如 for a, b in [(1,2), (3,4)]）if/else 条件块和条件表达式
  - 字符串格式（例如 '%.2f' % 3.14）
  - 变量、赋值、基本数据类型（int、float、bool、str）
  - pass 语句
- **中级 Python**
Python 教程还介绍了以下更高级的 Python 功能：
  - 列表推导式
  - Lambda 函数
 - **第三方 Python 库**
机器学习速成课程代码示例使用了第三方库提供的以下功能。无需提前熟悉这些库；您可以在需要时查询相关内容。
    - Matplotlib（适合数据可视化）
   - pyplot 模块
   - cm 模块
   - gridspec 模块
   - Seaborn（适合热图）
   - heatmap 函数
   - Pandas（适合数据处理）
   -  DataFrame 类
   - NumPy（适合低阶数学运算）
   - linspace 函数
   - random 函数
    - array 函数
   - arange 函数
   - scikit-learn（适合评估指标）
   - metrics 模块
#### Bash 终端/云端控制台
要在本地计算机上或云端控制台中运行编程练习，您应该能熟练使用命令行：
 - Bash 参考手册
-  Bash 快速参考表
- 了解 Shell

# 机器学习简介
本单元将为您介绍机器学习 (ML)。
- **预计用时：3 分钟**
- **学习目标**
	- 了解掌握机器学习技术的实际优势
  -	理解机器学习技术背后的理念

-	[视频讲座](https://cloud.189.cn/t/uMFnuaQfuqMn)
## 框架处理
### 视频讲座
本单元探讨了如何将某个任务构建为机器学习问题，并介绍了各种机器学习方法中通用的很多基本词汇术语。
- 学习目标
  - 复习机器学习基本术语。
  - 了解机器学习的各种用途。
 ### 机器学习主要术语
 什么是（监督式）机器学习？简单来说，它的定义如下：
 - 机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测。
下面我们来了解一下机器学习的基本术语。
#### 标签
**标签**是我们要预测的事物，即简单线性回归中的 y 变量。标签可以是小麦未来的价格、图片中显示的动物品种、音频剪辑的含义或任何事物。
#### 特征
**特征**是输入变量，即简单线性回归中的 x 变量。简单的机器学习项目可能会使用单个特征，而比较复杂的机器学习项目可能会使用数百万个特征，按如下方式指定：
> \{x_1, x_2, ... x_N\}

在垃圾邮件检测器示例中，特征可能包括：
- 电子邮件文本中的字词
- 发件人的地址
- 发送电子邮件的时段
- 电子邮件中包含“一种奇怪的把戏”这样的短语。                                    
#### 样本
**样本**是指数据的特定实例：x。（我们采用粗体 x 表示它是一个矢量。）我们将样本分为以下两类：
- 有标签样本
- 无标签样本
**有标签样本**同时包含特征和标签。即：
  > labeled examples: {features, label}: (x, y)
  
 我们使用有标签样本来训练模型。在我们的垃圾邮件检测器示例中，有标签样本是用户明确标记为“垃圾邮件”或“非垃圾邮件”的各个电子邮件。

例如，下表显示了从包含加利福尼亚州房价信息的数据集中抽取的 5 个有标签样本：

| housingMedianAge |  totalRooms| totalBedrooms | medianHouseValue |
|--|--|--|--|
|15  | 5612 | 1283 |66900  |
|--|--|--|--|
| 19 |7650  | 7650 | 80100 |
|--|--|--|--|
|17 |720  |174  |85700  |
|--|--|--|--|
|14| 1501 |337  | 73400 |
|--|--|--|--|
| 20 | 1454 | 326 | 65500 |
**无标签样**本包含特征，但不包含标签。即：
 >  unlabeled examples: {features, ?}: (x, ?)

在使用有标签样本训练了我们的模型之后，我们会使用该模型来预测无标签样本的标签。在垃圾邮件检测器示例中，无标签样本是用户尚未添加标签的新电子邮件。

#### 模型
模型定义了特征与标签之间的关系。例如，垃圾邮件检测模型可能会将某些特征与“垃圾邮件”紧密联系起来。我们来重点介绍一下模型生命周期的两个阶段：

- 训练表示创建或学习模型。也就是说，您向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。

- 推断表示将训练后的模型应用于无标签样本。也就是说，您使用训练后的模型来做出有用的预测 (y')。例如，在推断期间，您可以针对新的无标签样本预测 medianHouseValue。

#### 回归与分类
**回归模型**可预测连续值。例如，回归模型做出的预测可回答如下问题：
- 加利福尼亚州一栋房产的价值是多少？
- 用户点击此广告的概率是多少？

**分类模型**可预测离散值。例如，分类模型做出的预测可回答如下问题：
- 某个指定电子邮件是垃圾邮件还是非垃圾邮件？
- 这是一张狗、猫还是仓鼠图片？
> 关键字词
> >   分类模型  样本   特征  推断   标签   模型   回归 训练

## 深入了解机器学习
### 线性回归
人们早就知晓，相比凉爽的天气，蟋蟀在较为炎热的天气里鸣叫更为频繁。数十年来，专业和业余昆虫学者已将每分钟的鸣叫声和温度方面的数据编入目录。Ruth 阿姨将她喜爱的蟋蟀数据库作为生日礼物送给您，并邀请您自己利用该数据库训练一个模型，从而预测鸣叫声与温度的关系。

首先建议您将数据绘制成图表，了解下数据的分布情况：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20181127224750550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

**图 1. 每分钟的鸣叫声与温度（摄氏度）的关系。**

毫无疑问，此曲线图表明温度随着鸣叫声次数的增加而上升。鸣叫声与温度之间的关系是线性关系吗？是的，您可以绘制一条直线来近似地表示这种关系，如下所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181127224925912.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

**图 2. 线性关系。**

事实上，虽然该直线并未精确无误地经过每个点，但针对我们拥有的数据，清楚地显示了鸣叫声与温度之间的关系。只需运用一点代数知识，您就可以将这种关系写下来，如下所示：
>$y' = b + w_1x_1$
其中：

 - $y'$指的是预测标签（理想输出值）。
 - $b$ 指的是偏差（y 轴截距）。而在一些机器学习文档中，它称为 。
 - $w_1$指的是特征 1 的权重。权重与上文中用  表示的“斜率”的概念相同。
 - $x_1$指的是特征（已知输入项）。
要根据新的每分钟的鸣叫声值  推断（预测）温度 ，只需将  值代入此模型即可。

下标（例如$w_1$  和 $x_1$ ）预示着可以用多个特征来表示更复杂的模型。例如，具有三个特征的模型可以采用以下方程式：
 > $y' = b + w_1x_1 + w_2x_2 + w_3x_3$
 ### 训练与损失
简单来说，**训练**模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值。在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度地减少损失的模型；这一过程称为经验风险最小化。

损失是对糟糕预测的惩罚。也就是说，损失是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。例如，图 3 左侧显示的是损失较大的模型，右侧显示的是损失较小的模型。关于此图，请注意以下几点：

- 红色箭头表示损失。
- 蓝线表示预测。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181127225814208.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

 
左侧模型的损失较大；右侧模型的损失较小。

请注意，左侧曲线图中的红色箭头比右侧曲线图中的对应红色箭头长得多。显然，相较于左侧曲线图中的蓝线，右侧曲线图中的蓝线代表的是预测效果更好的模型。

您可能想知道自己能否创建一个数学函数（损失函数），以有意义的方式汇总各个损失。

平方损失：一种常见的损失函数
接下来我们要看的线性回归模型使用的是一种称为平方损失（又称为 L2 损失）的损失函数。单个样本的平方损失如下：
  >= the square of the difference between the label and the prediction
  = (observation - prediction(x))2
  = (y - y')2
  
 **均方误差 (MSE)** 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：
 $MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2$

 其中：
 - $(x, y)$指的是样本，其中
   - $x$ 指的是模型进行预测时使用的特征集（例如，温度、年龄和交配成功率）。
    - $y$ 指的是样本的标签（例如，每分钟的鸣叫次数）。
   - $prediction(x)$ 指的是权重和偏差与特征集  结合的函数。
   - $D$ 指的是包含多个有标签样本（即 ）的数据集。
   - $N$ 指的是 $D$ 中的样本数量。

虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。

## 降低损失
### 迭代方法
上一单元介绍了损失的概念。在本单元中，您将了解机器学习模型如何以迭代方式降低损失。

迭代学习可能会让您想到“Hot and Cold”这种寻找隐藏物品（如顶针）的儿童游戏。在我们的游戏中，“隐藏的物品”就是最佳模型。刚开始，您会胡乱猜测（“ $w_1$的值为 0。”），等待系统告诉您损失是多少。然后，您再尝试另一种猜测（“$w_1$ 的值为 0.5。”），看看损失是多少。哎呀，这次更接近目标了。实际上，如果您以正确方式玩这个游戏，通常会越来越接近目标。这个游戏真正棘手的地方在于尽可能高效地找到最佳模型。

下图显示了机器学习算法用于训练模型的迭代试错过程：

 ![降低损失](https://img-blog.csdnimg.cn/20181128074244372.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)	

图 1. 用于训练模型的迭代方法。

我们将在整个机器学习速成课程中使用相同的迭代方法详细说明各种复杂情况，尤其是处于暴风雨中的蓝云区域。迭代策略在机器学习中的应用非常普遍，这主要是因为它们可以很好地扩展到大型数据集。

“模型”部分将一个或多个特征作为输入，然后返回一个预测 (y') 作为输出。为了进行简化，不妨考虑一种采用一个特征并返回一个预测的模型：
> $y' = b + w_1x_1$

我们应该为 $b$和 $w_1$ 设置哪些初始值？对于线性回归问题，事实证明初始值并不重要。我们可以随机选择值，不过我们还是选择采用以下这些无关紧要的值：
 - $b$ = 0
 - $w_1$ = 0
假设第一个特征值是 10。将该特征值代入预测函数会得到以下结果：
>   y' = 0 + 0(10)
  y' = 0
 
 图中的“计算损失”部分是模型将要使用的损失函数。假设我们使用平方损失函数。损失函数将采用两个输入值：

- y'：模型对特征 x 的预测
- y：特征 x 对应的正确标签。

最后，我们来看图的“计算参数更新”部分。机器学习系统就是在此部分检查损失函数的值，并为$b$ 和 $w_1$生成新值。现在，假设这个神秘的绿色框会产生新值，然后机器学习系统将根据所有标签重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。这种学习过程会持续迭代，直到该算法发现损失可能最低的模型参数。通常，您可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已收敛。
> **要点**
在训练机器学习模型时，首先对权重和偏差进行初始猜测，然后反复调整这些猜测，直到获得损失可能最低的权重和偏差为止。

>  **关键字词  收敛  损失  训练**

### 梯度下降法
迭代方法图（图 1）包含一个标题为“计算参数更新”的华而不实的绿框。现在，我们将用更实质的方法代替这种华而不实的算法。

假设我们有时间和计算资源来计算  的所有可能值的损失。对于我们一直在研究的回归问题，所产生的损失与  的图形始终是凸形。换言之，$w$图形始终是碗状图，如下所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129212906968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

**图 2. 回归问题产生的损失与权重图为凸形。**

 

凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处。

通过计算整个数据集中  每个可能值的损失函数来找到收敛点这种方法效率太低。我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为**梯度下降法**。

梯度下降法的第一个阶段是为  选择一个起始值（起点）。起点并不重要；因此很多算法就直接将  设为 0 或随机选择一个值。下图显示的是我们选择了一个稍大于 0 的起点：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129213016975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

图 3. 梯度下降法的起点。

然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，梯度是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度（如图 3 所示）就等于导数。

详细了解偏导数和梯度。
涉及机器学习领域的数学非常有趣，我们很高兴您点击了该链接来了解详情。不过请注意，TensorFlow 会为您处理所有的梯度计算过程，因此您其实不必理解此处提供的微积分知识。

>偏导数
多变量函数指的是具有多个参数的函数，例如：
$f(x,y) = e^{2y}sin(x)$
f 相对于 x 的偏导数表示如下：
$\partial f \over \partial x$ 
是 f (x) 的导数。要计算以下值：
 $e^2cos(x)$
 一般来说，假设 y 保持不变，f 对 x 的偏导数的计算公式如下：
 $\frac{\partial f}{\partial x}(x,y) = e^{2y}cos(x)$
 同样，如果我们使 x 保持不变，f 对 y 的偏导数为：
$\frac{\partial f}{\partial y}(x,y) = 2e^{2y}sin(x)$
直观而言，偏导数可以让您了解到，当您略微改动一个变量时，函数会发生多大的变化。在前面的示例中：
$\frac{\partial f}{\partial x} (0,1) = e^2 \approx 7.4$
因此，如果您将起点设为 (0,1)，使 y 保持固定不变并将 x 移动一点，f 的变化量将是 x 变化量的 7.4 倍左右。

在机器学习中，偏导数主要与函数的梯度一起使用。

梯度
函数的梯度是偏导数相对于所有自变量的矢量，表示如下：
$\nabla f$
例如，如果：
$f(x,y) = e^{2y}sin(x)$
则：
$\nabla f(x,y) = (\frac{\partial f}{\partial x}(x,y), \frac{\partial f}{\partial y}(x,y)) = (e^{2y}cos(x), 2e^{2y}sin(x))$
请注意以下几点：
$\nabla f$  指向函数增长速度最快的方向。
${-\nabla f}$ 指向函数下降速度最快的方向。
该矢量中的维度个数等于 f 公式中的变量个数；换言之，该矢量位于该函数的域空间内。例如，在三维空间中查看下面的函数 f(x,y) 时：
$f(x,y) = 4 + (x - 2)^2 + 2y^2$
z = f(x,y) 就像一个山谷，最低点为 (2,0,4)：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129214349199.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)    >  f(x,y) 的梯度是一个二维矢量，可让您了解向哪个 (x,y) 方向移动时高度下降得最快。也就是说，梯度矢量指向山谷。
在机器学习中，梯度用于梯度下降法。我们的损失函数通常具有很多变量，而我们尝试通过跟随函数梯度的负方向来尽量降低损失函数。

请注意，梯度是一个矢量，因此具有以下两个特征：

- 方向
- 大小
梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失
![在这里插入图片描述](https://img-blog.csdnimg.cn/2018112921452939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

**图 4. 梯度下降法依赖于负梯度**

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加，如下图所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129214659573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

图 5. 一个梯度步长将我们移动到损失曲线上的下一个点。

然后，梯度下降法会重复此过程，逐渐接近最低点。
> 关键字词
>>梯度下降法 
>>步

### 学习速率
正如之前所述，梯度矢量具有方向和大小。梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。

超参数是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果您选择的学习速率过小，就会花费太长的学习时间：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129215530971.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

图 6. 学习速率过小。

相反，如果您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳，就好像量子力学实验出现了严重错误一样：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129215603651.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

图 7. 学习速率过大。

每个回归问题都存在一个金发姑娘学习速率。“金发姑娘”值与损失函数的平坦程度相关。如果您知道损失函数的梯度较小，则可以放心地试着采用更大的学习速率，以补偿较小的梯度并获得更大的步长。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129215637392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

**图 8. 学习速率恰恰好。**
详细了解理想的学习速率。
> 一维空间中的理想学习速率是$\frac{ 1 }{ f(x)'' }$（f(x) 对 x 的二阶导数的倒数）。
二维或多维空间中的理想学习速率是海森矩阵（由二阶偏导数组成的矩阵）的倒数。
广义凸函数的情况则更为复杂。

> 关键字词
>> 超参数
>> 学习速率
>> 步长
### 优化学习速率
尝试不同的学习速率，看看不同的学习速率对到达损失曲线最低点所需的步数有何影响。请尝试进行图表下方的练习。

优化学习速率
预计用时：15 分钟

#### 练习 1
在滑块上设置 0.1 的学习速率。不断按下（步数）按钮，直到梯度下降法算法到达损失曲线的最低点。一共走了多少步？

#### 练习 2
您可以使用更高的学习速率更快地到达最低点吗？将学习速率设为 1，然后不断按（步数）按钮，直到梯度下降法到达最低点。这次走了多少步？
#### 练习 3
如果采用更大的学习速率会怎么样？重置该图，将学习速率设为 4，然后尝试到达损失曲线的最低点。这次发生了什么情况？
#### 可选挑战
您能否为该曲线找到金发姑娘般刚刚好的学习速率，让梯度下降法以最少的步数到达最低点？最少需要多少步才能到达最低点？
### 随机梯度下降法
在梯度下降法中，批量指的是用于在单次迭代中计算梯度的样本总数。到目前为止，我们一直假定批量是指整个数据集。就 Google 的规模而言，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含海量特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算。

包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。

如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 随机梯度下降法 (SGD) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

为了简化说明，我们只针对单个特征重点介绍了梯度下降法。请放心，梯度下降法也适用于包含多个特征的特征集。

### Playground 练习
学习速率和收敛
这是一系列 Playground 练习中的第一个练习。 Playground 是专为本课程开发的教程，旨在讲解机器学习原理。

每个 Playground 练习都会生成一个数据集。此数据集的标签具有两个可能值。您可以将这两个可能值设想成垃圾邮件与非垃圾邮件，或者设想成健康的树与生病的树。大部分练习的目标是调整各种超参数，以构建可成功划分（分开或区分）一个标签值和另一个标签值的模型。请注意，大部分数据集都包含一定数量的杂乱样本，导致无法成功划分每个样本。
> 每个 Playground 练习都会显示模型当前状态的直观图示。例如，以下就是一个模型的直观图示：
> 请注意以下关于模型直观图示的说明：
每个蓝点表示一类数据的一个样本（例如，一棵健康的树）。
每个橙点表示另一类数据的一个样本（例如，一棵生病的树）。
背景颜色表示该模型对于应该在何处找到相应颜色样本的预测。某个蓝点周围显示蓝色背景表示该模型正确地预测了该样本。相反，某个蓝点周围显示橙色背景则表示该模型错误地预测了该样本。
背景的蓝色和橙色部分色调会有深浅之分。例如，直观图示的左侧是纯蓝色，但在直观图示的中心颜色则逐渐淡化为白色。您可以将颜色强度视为表明该模型对其猜测结果的自信程度。因此，纯蓝色表示该模型对其猜测结果非常自信，而浅蓝色则表示该模型的自信程度稍低。（图中所示的模型直观图示在预测方面的表现非常糟糕。）
可以通过直观图示来判断模型的进展。（“非常棒 - 大多数蓝点都有蓝色背景”或者“糟糕！蓝点有橙色背景。”）除了颜色之外，Playground 还会以数字形式显示模型的当前损失。（“糟糕！损失正在上升，而不是下降。”）
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129224606178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

在这第一个 Playground 练习中，您将通过执行以下两个任务来尝试不同的学习速率。

#### 任务 1：
注意 Playgroud 右上角的学习速率菜单。指定学习速率为 3，这个值非常高。通过点击“步”按钮 10 或 20 次，观察这种较高的学习速率会如何影响您的模型。在早期的每次迭代之后，请注意模型的直观图示如何急剧变化。模型似乎已收敛后，您甚至可能看到出现不稳定的情况。另请注意从 x1 和 x2 到模型直观图示之间的线。这些线的权重表示模型中相应特征的权重。也就是说，线越粗，权重越高。

#### 任务 2：
执行以下操作：
1. 按重置按钮。
2. 降低学习速率。
3. 多次按“步”按钮。

较低的学习速率对收敛有何影响？了解模型收敛所需的步数，并了解模型收敛的顺滑平稳程度。尝试较低的学习速率。能否发现因过慢而无用的学习速率？（您将在练习的正下方找到相关讨论。）
- 击即可查看关于任务 2 的讨论。
Playground 练习具有非确定性，我们无法始终提供与您的数据集完全一致的答案。即便如此，对于我们来说，0.1 的学习速率仍可有效收敛。学习速率越小，收敛所花费的时间越多；也就是说，较小的学习速率会因过慢而没有用处。
## 使用 TensorFlow 的起始步骤
### 学习目标
- 了解如何在 TensorFlow 中创建和修改张量。
- 了解 Pandas 的基础知识。
- 使用 TensorFlow 的一种高级 API 开发线性回归代码。
- 尝试不同的学习速率。
### 使用 TensorFlow 的起始步骤(工具包）
下图显示了 TensorFlow 工具包的当前层次结构：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129225540338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

图 1. TensorFlow 工具包层次结构。

下表总结了不同层的用途：
|工具包	  |  说明|
|--|--|
|Estimator (tf.estimator)  |	高级 OOP API。  |
|tf.layers/tf.losses/tf.metrics  | 	用于常见模型组件的库。 |
| TensorFlow | 低级 API
 TensorFlow 由以下两个组件组成：
 - 图协议缓冲区
- 执行（分布式）图的运行时


这两个组件类似于 Java 编译器和 JVM。正如 JVM 会实施在多个硬件平台（CPU 和 GPU）上一样，TensorFlow 也是如此。

您应该使用哪个 API？您应该使用能够解决问题的最高级抽象层。较高级别的抽象层更易于使用，但（设计方面）不够灵活。我们建议您先从最高级 API 入手，让所有组件正常运作起来。如果您希望在某些特殊建模方面能够更加灵活一些，则可以降低一个级别。请注意，每个级别都是使用低级 API 构建的，因此降低层次结构级别应该比较直观。|

### tf.estimator API
我们将使用 tf.estimator 来完成机器学习速成课程中的大部分练习。您在练习中所做的一切都可以在较低级别（原始）的 TensorFlow 中完成，但使用 tf.estimator 会大大减少代码行数。

tf.estimator 与 scikit-learn API 兼容。 scikit-learn 是极其热门的 Python 开放源代码机器学习库，拥有超过 10 万名用户，其中包括许多 Google 员工。

概括而言，以下是在 tf.estimator 中实现的线性回归程序的格式：
  > import tensorflow as tf
 \# Set up a linear classifier.
 classifier = tf.estimator.LinearClassifier()
\# Train the model on some example data.
classifier.train(input_fn=train_input_fn, steps=2000)
\# Use it to predict.
predictions = classifier.predict(input_fn=predict_input_fn)
### 关键字词
- Estimator
- 图
- 张量
### 编程练习
机器学习速成课程会逐步引导您使用 tf.estimator（一种高级 TensorFlow API）对模型进行编码，以便将学到的原则和技术应用于实践。

机器学习速成课程中的编程练习使用的是可将代码、输出和说明性文字合并到一个协作文档中的数据分析平台。

> 可使用 Colaboratory 平台直接在浏览器中运行编程练习（无需设置！）。Colaboratory 支持大多数主流浏览器，并且在 Chrome 和 Firefox 的各个桌面版本上进行了最全面的测试。如果您想下载并离线运行这些练习，请参阅有关设置本地环境的说明。

请按照指定顺序运行以下三个练习：

1. Pandas 简介：Pandas 是用于进行数据分析和建模的重要库，广泛应用于 TensorFlow 编码。该教程提供了学习本课程所需的全部 Pandas 信息。如果您已了解 Pandas，则可以跳过此练习。
2. 使用 TensorFlow 的起始步骤：此练习介绍了线性回归。
3. 合成特征和离群值：此练习介绍了合成特征以及输入离群值带来的影响。
#### 机器学习速成课程练习中的常用超参数
很多编码练习都包含以下超参数：

- steps：训练迭代的总次数。一步计算一批样本产生的损失，然后使用该值修改一次模型的权重。
- batch size：单步的样本数量（随机选择）。例如，SGD 的批次大小为 1。
以下公式成立：
$total\,number\,of\,trained\,examples = batch\,size * steps$
#### 机器学习速成课程练习中的方便变量
有些练习中会出现以下方便变量：

- periods：控制报告的粒度。例如，如果 periods 设为 7 且 steps 设为 70，则练习将每 10 步输出一次损失值（即 7 次）。与超参数不同，我们不希望您修改 periods 的值。请注意，修改 periods 不会更改模型所学习的规律。
以下公式成立：
$number\,of\,training\,examples\,in\,each\,period = \frac{batch\,size * steps} {periods}$
## 泛化 (Generalization)
### 学习目标
- 直观理解过拟合。
- 确定某个模型是否出色。
- 将数据集划分为训练集和测试集。
 
### 过拟合的风险
本单元将重点介绍泛化。为了让您直观地理解这一概念，我们将展示 3 张图。假设这些图中的每个点代表一棵树在森林中的位置。图中的两种颜色分别代表以下含义：

蓝点代表生病的树。
橙点代表健康的树。
接下来，我们来看看图 1。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2018112923113918.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)
**图 1. 生病（蓝色）和健康（橙色）的树。**

您能设想出一个有效的模型来预测以后的生病或健康的树吗？花点时间在脑海里绘制一条弧线将蓝点与橙点分开，或者在脑海中圈住一些橙点或蓝点。然后再看看图 2，它显示某种机器学习模型如何将生病的树与健康的树区分开。请注意，该模型产生的损失非常低。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129231231855.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)
图 2. 用于区分生病的树与健康的树的复杂模型。

乍一看，图 2 所示的模型在将健康的树与生病的树区分开方面似乎表现得非常出色。真的是这样吗？

损失很低，但仍然是糟糕的模型？
图 3 显示我们向该模型中添加了新数据后所发生的情况。结果表明，该模型在处理新数据方面表现非常糟糕。请注意，该模型对大部分新数据的分类都不正确。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20181129231342126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODk3ODc0MQ==,size_16,color_FFFFFF,t_70)

**图 3. 该模型在预测新数据方面表现非常糟糕。**

图 2 和图 3 所示的模型过拟合了训练数据的特性。过拟合模型在训练过程中产生的损失很低，但在预测新数据方面的表现却非常糟糕。如果某个模型在拟合当前样本方面表现良好，那么我们如何相信该模型会对新数据做出良好的预测呢？正如您稍后将看到的，过拟合是由于模型的复杂程度超出所需程度而造成的。机器学习的基本冲突是适当拟合我们的数据，但也要尽可能简单地拟合数据。

机器学习的目标是对从真实概率分布（已隐藏）中抽取的新数据做出良好预测。遗憾的是，模型无法查看整体情况；模型只能从训练数据集中取样。如果某个模型在拟合当前样本方面表现良好，那么您如何相信该模型也会对从未见过的样本做出良好预测呢？

奥卡姆的威廉是 14 世纪一位崇尚简单的修士和哲学家。他认为科学家应该优先采用更简单（而非更复杂）的公式或理论。奥卡姆剃刀定律在机器学习方面的运用如下：
> 机器学习模型越简单，良好的实证结果就越有可能不仅仅基于样本的特性。

现今，我们已将奥卡姆剃刀定律正式应用于**统计学习理论**和**计算学习理论**领域。这些领域已经形成了泛化边界，即统计化描述模型根据以下因素泛化到新数据的能力：
- 模型的复杂程度
- 模型在处理训练数据方面的表现

虽然理论分析在理想化假设下可提供正式保证，但在实践中却很难应用。机器学习速成课程则侧重于实证评估，以评判模型泛化到新数据的能力。

机器学习模型旨在根据以前未见过的新数据做出良好预测。但是，如果您要根据数据集构建模型，如何获得以前未见过的数据呢？一种方法是将您的数据集分成两个子集：

- 训练集 - 用于训练模型的子集。
- 测试集 - 用于测试模型的子集。

一般来说，在测试集上表现是否良好是衡量能否在新数据上表现良好的有用指标，前提是：

测试集足够大。
您不会反复使用相同的测试集来作假。

### 机器学习细则
以下三项基本假设阐明了泛化：
- 我们从分布中随机抽取独立同分布 (i.i.d) 的样本。换言之，样本之间不会互相影响。（另一种解释：i.i.d. 是表示变量随机性的一种方式）。
- 分布是平稳的；即分布在数据集内不会发生变化。
- 我们从同一分布的数据划分中抽取样本。

在实践中，我们有时会违背这些假设。例如：
- 想象有一个选择要展示的广告的模型。如果该模型在某种程度上根据用户以前看过的广告选择广告，则会违背 i.i.d. 假设。
- 想象有一个包含一年零售信息的数据集。用户的购买行为会出现季节性变化，这会违反平稳性。
- 如果违背了上述三项基本假设中的任何一项，那么我们就必须密切注意指标。

> **总结**
>> - 如果某个模型尝试紧密拟合训练数据，但却不能很好地泛化到新数据，就会发生过拟合。
>> - 如果不符合监督式机器学习的关键假设，那么我们将失去对新数据进行预测这项能力的重要理论保证。

#### 关键字词
>> - 泛化  
>> - 过拟合
>> - 预测
>> - 平稳性
>> - 测试集
>> - 训练集
